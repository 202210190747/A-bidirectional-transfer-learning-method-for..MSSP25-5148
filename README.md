# A-bidirectional-transfer-learning-method-for..MSSP25-5148
Includes the code, public datasets, and robot milling data

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import time
from datetime import datetime

# ========================== 1. æ ¸å¿ƒå‚æ•°é…ç½® ==========================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# æ•°æ®é›†è·¯å¾„
datasets = {
    'C1': 'filtered_data_normalized-C1.csv',
    'C4': 'filtered_data_normalized-C4.csv',
    'C6': 'filtered_data_normalized-C6.csv',
    'RC1': 'filtered_data_normalized-ROBOT-C1.csv',
    'RC2': 'filtered_data_normalized-ROBOT-C2.csv',
    'RC3': 'filtered_data_normalized-ROBOT-C3.csv'
}

# å®éªŒé…ç½®ï¼š6 ç»„æºâ†’ç›®æ ‡
experiments = [
    {'name': 'C1+C4+C6â†’RC1', 'source_keys': ['C1', 'C4', 'C6'], 'target_key': 'RC1'},
    {'name': 'C1+C4+C6â†’RC2', 'source_keys': ['C1', 'C4', 'C6'], 'target_key': 'RC2'},
    {'name': 'C1+C4+C6â†’RC3', 'source_keys': ['C1', 'C4', 'C6'], 'target_key': 'RC3'},
]

# è®­ç»ƒè½®æ•°ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰
source_epochs = 120   # æºåŸŸé¢„è®­ç»ƒè½®æ•°
target_epochs = 120   # ç›®æ ‡åŸŸå¾®è°ƒè½®æ•°

# å…¶ä»–è®­ç»ƒå‚æ•°
batch_size = 64
learning_rate = 0.001
alpha = 0.01      # PINN ç‰©ç†æŸå¤±æƒé‡
beta = 0.01       # å•è°ƒæ€§æŸå¤±æƒé‡
loss_fn = nn.MSELoss()

# æ—©åœé…ç½®ï¼ˆåªä½œç”¨äº fine_tuning é˜¶æ®µï¼‰
EARLY_STOPPING = True
PATIENCE = 10        # è¿ç»­å¤šå°‘æ¬¡è¯„ä¼°æ— æå‡å°±åœæ­¢
MIN_DELTA = 0.0      # RMSE è‡³å°‘æå‡å¤šå°‘æ‰ç®—â€œæœ‰è¿›æ­¥â€
EVAL_INTERVAL = 5    # æ¯å¤šå°‘ä¸ª epoch åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ä¸€æ¬¡


# ========================== 2. æ•°æ®å¤„ç†å·¥å…·å‡½æ•° ==========================
def load_single_dataset(dataset_key):
    """æ ¹æ® key è¯»å•ä¸ªæ•°æ®é›†ï¼Œé»˜è®¤æœ€åä¸€åˆ—ä¸º RUL æ ‡ç­¾"""
    try:
        data = pd.read_csv(datasets[dataset_key])
        if data.shape[1] < 2:
            raise ValueError(f"æ•°æ®é›†{dataset_key}æ ¼å¼é”™è¯¯ï¼Œéœ€è‡³å°‘1ç‰¹å¾+1æ ‡ç­¾")
        print(f"âœ… åŠ è½½æ•°æ®é›†: {datasets[dataset_key]} | æ ·æœ¬æ•°: {len(data)} | ç‰¹å¾æ•°: {data.shape[1]-1}")
        return data
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶: {datasets[dataset_key]}")
        exit()


def add_time_feature(data):
    """è‹¥æ—  Time åˆ—ï¼Œåˆ™æ’å…¥ä¸€ä¸ªå½’ä¸€åŒ–æ—¶é—´ç‰¹å¾ [0,1]"""
    if 'Time' not in data.columns:
        data.insert(0, 'Time', np.linspace(0, 1, len(data)))
    return data


def merge_source_datasets(source_keys):
    """æŒ‰æºåŸŸåˆ—è¡¨åˆå¹¶æ•°æ®é›†"""
    merged_X, merged_y = None, None
    for key in source_keys:
        data = load_single_dataset(key)
        data = add_time_feature(data)
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values
        if merged_X is None:
            merged_X, merged_y = X, y
        else:
            merged_X = np.concatenate([merged_X, X], axis=0)
            merged_y = np.concatenate([merged_y, y], axis=0)
    print(f"ğŸ“Š åˆå¹¶æºåŸŸ {source_keys} | æ€»æ ·æœ¬æ•°: {len(merged_X)} | ç‰¹å¾æ•°: {merged_X.shape[1]}")
    return merged_X, merged_y


def split_target_data(X_target, y_target):
    """
    ç›®æ ‡åŸŸåˆ’åˆ†ï¼š
      - æµ‹è¯•é›†ï¼šæ¯éš” 5 ä¸ªæ ·æœ¬å– 1 ä¸ªï¼ˆçº¦ 20%ï¼‰
      - å¾®è°ƒé›†ï¼šåœ¨å‰©ä½™æ ·æœ¬ä¸­æ¯éš” 8 ä¸ªå– 1 ä¸ª
    """
    total_indices = np.arange(len(X_target))

    # æµ‹è¯•é›†
    test_indices = total_indices[::5]
    X_test = X_target[test_indices]
    y_test = y_target[test_indices]

    # å¾®è°ƒé›†
    remaining_indices = np.setdiff1d(total_indices, test_indices)
    if len(remaining_indices) < 8:
        ft_indices = remaining_indices
    else:
        ft_indices = remaining_indices[::8]

    X_ft = X_target[ft_indices]
    y_ft = y_target[ft_indices]

    print(f"ğŸ¯ ç›®æ ‡åŸŸé‡‡æ · | å¾®è°ƒé›†: {len(X_ft)} æ ·æœ¬ | æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    return X_ft, y_ft, X_test, y_test


def prepare_tensors(data, device):
    """è½¬æ¢ä¸ºæ—¶åºæ¨¡å‹æ ‡å‡†æ ¼å¼ï¼š[æ ·æœ¬æ•°, seq_len=1, ç‰¹å¾æ•°]"""
    tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(1)
    return tensor.to(device)


# ========================== 3. BLCAP æ¨¡å‹å®šä¹‰ï¼ˆBaseModelï¼‰ ==========================
class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼šå¯¹ BiLSTM è¾“å‡ºçš„é€šé“è¿›è¡ŒåŠ æƒ"""
    def __init__(self, hidden_size, reduction_ratio=4):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // reduction_ratio, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_size // reduction_ratio, hidden_size, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x: [batch, seq_len, hidden_size]
        b, l, c = x.size()
        y = self.avg_pool(x.permute(0, 2, 1)).view(b, c)  # é€šé“å…¨å±€æ± åŒ–
        y = self.fc(y).view(b, 1, c)                      # ç”Ÿæˆé€šé“æƒé‡
        return x * y.expand_as(x)                         # æ–½åŠ æ³¨æ„åŠ›æƒé‡


class BaseModel(nn.Module):
    """
    BLCAP ä¸»å¹²ï¼š
      - BiLSTM æå–æ—¶åºç‰¹å¾
      - ChannelAttention å»ºæ¨¡ä¸åŒéšé€šé“çš„é‡è¦æ€§
      - å…¨è¿æ¥å›å½’ RULï¼ˆ0~1ï¼‰
      - physics_loss + monotonicity_loss ç‰©ç†/å•è°ƒçº¦æŸ
    """
    def __init__(self, input_size, hidden_size=64):
        super().__init__()
        self.hidden_size = hidden_size
        self.bilstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            bidirectional=True,
            batch_first=True
        )
        self.channel_attn = ChannelAttention(hidden_size * 2)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 2, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        self.time_idx = 0  # å‡è®¾ç¬¬ 0 ç»´æ˜¯ Time ç‰¹å¾

    def forward(self, x):
        # x: [batch, seq_len, features]
        x, _ = self.bilstm(x)           # [batch, seq_len, hidden_size*2]
        x = self.channel_attn(x)
        return self.fc(x[:, -1, :])     # å–æœ€åæ—¶é—´æ­¥è¾“å‡º

    def physics_loss(self, y_pred, x):
        """
        PINN ç‰©ç†çº¦æŸï¼šRUL éšæ—¶é—´é€’å‡ï¼Œç†æƒ³å…³ç³»ï¼šy(t) <= 1 - t
        ç”¨ ReLU å¼ºåˆ¶å¯¹è¿åçº¦æŸçš„éƒ¨åˆ†è¿›è¡Œæƒ©ç½šã€‚
        """
        t = x[:, :, self.time_idx].squeeze()  # [batch] æˆ– [batch, seq_len]
        return torch.mean(torch.relu(y_pred.squeeze() - (1 - t)))

    def monotonicity_loss(self, y_pred, x):
        """
        å•è°ƒæ€§çº¦æŸï¼šéšæ—¶é—´ t å¢å¤§ï¼ŒRUL ä¸èƒ½ä¸Šå‡ã€‚
        å°†æ ·æœ¬æŒ‰ t æ’åºï¼Œçº¦æŸç›¸é‚»å·®åˆ† y_{i+1} - y_i <= 0ã€‚
        """
        t = x[:, :, self.time_idx].squeeze()
        sorted_idx = torch.argsort(t)
        sorted_pred = y_pred[sorted_idx]
        diffs = sorted_pred[1:] - sorted_pred[:-1]
        return torch.mean(torch.relu(diffs))


# ========================== 4. è¯„ä¼°ä¸è®­ç»ƒå‡½æ•° ==========================
def evaluate_model(model, X_tensor, y_tensor, return_time=False):
    """åœ¨ç»™å®šæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå¯é€‰è¿”å›ä¸€æ¬¡å®Œæ•´æ¨ç†æ—¶é—´"""
    model.eval()
    start_time = time.time()
    with torch.no_grad():
        y_pred = model(X_tensor).cpu().numpy().flatten()
        y_true = y_tensor.cpu().numpy().flatten()
    infer_time = time.time() - start_time

    metrics = {
        'true_rul': y_true,
        'pred_rul': y_pred,
        'mae': mean_absolute_error(y_true, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
        'r2': r2_score(y_true, y_pred)
    }

    if return_time:
        return metrics, infer_time
    else:
        return metrics


def pretrain_source_model(model, X_train, y_train, device, epochs):
    """æºåŸŸé¢„è®­ç»ƒï¼ˆBLCAP ä¸»å¹²ï¼‰ï¼Œä¸åŠ æ—©åœï¼Œç›´æ¥è·‘æ»¡ epochs"""
    dataset = TensorDataset(X_train, y_train.unsqueeze(1))
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)

    start_time = time.time()
    for epoch in range(epochs):
        model.train()
        total_loss = 0.0

        for batch_x, batch_y in loader:
            optimizer.zero_grad()
            y_pred = model(batch_x)
            data_loss = loss_fn(y_pred, batch_y)
            pde_loss = model.physics_loss(y_pred, batch_x)
            mono_loss = model.monotonicity_loss(y_pred, batch_x)
            loss = data_loss + alpha * pde_loss + beta * mono_loss

            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item() * batch_x.size(0)

        if (epoch + 1) % 5 == 0:
            avg_loss = total_loss / len(loader.dataset)
            print(f"  æºåŸŸé¢„è®­ç»ƒ Epoch [{epoch+1}/{epochs}] | Loss: {avg_loss:.4f}")

    pretrain_time = time.time() - start_time
    print(f"ğŸ“Œ æºåŸŸé¢„è®­ç»ƒå®Œæˆ | è€—æ—¶: {pretrain_time:.2f} ç§’")
    return model, pretrain_time


def train_fine_tuning(source_model, target_model,
                      X_ft, y_ft, X_test, y_test,
                      device, target_epochs,
                      early_stopping=EARLY_STOPPING,
                      patience=PATIENCE,
                      min_delta=MIN_DELTA,
                      eval_interval=EVAL_INTERVAL):
    """
    BLCAP + fine_tuningï¼š
      - å…ˆåŠ è½½æºåŸŸé¢„è®­ç»ƒæƒé‡
      - åœ¨ç›®æ ‡åŸŸå¾®è°ƒï¼ˆå¸¦ç‰©ç†/å•è°ƒæŸå¤±ï¼‰
      - æŒ‰ç›®æ ‡åŸŸ Test RMSE æ—©åœ
    è¿”å›ï¼š(æœ€ç»ˆè¯„ä¼°ç»“æœ, å¾®è°ƒè®­ç»ƒè€—æ—¶, æ¨ç†è€—æ—¶)
    """
    print(f"\nğŸ“Œ å¾®è°ƒè¿ç§»å­¦ä¹ è®­ç»ƒ (BLCAP + Fine-Tuning)ï¼Œç›®æ ‡åŸŸ epochs={target_epochs}")
    target_model.load_state_dict(source_model.state_dict())

    dataset = TensorDataset(X_ft, y_ft.unsqueeze(1))
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = optim.AdamW(target_model.parameters(), lr=learning_rate / 10, weight_decay=1e-4)

    start_time = time.time()
    best_rmse = float('inf')
    best_state = None
    epochs_no_improve = 0

    for epoch in range(target_epochs):
        target_model.train()
        total_loss = 0.0

        for batch_x, batch_y in loader:
            optimizer.zero_grad()
            y_pred = target_model(batch_x)
            data_loss = loss_fn(y_pred, batch_y)
            pde_loss = target_model.physics_loss(y_pred, batch_x)
            mono_loss = target_model.monotonicity_loss(y_pred, batch_x)
            loss = data_loss + alpha * pde_loss + beta * mono_loss

            loss.backward()
            nn.utils.clip_grad_norm_(target_model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item() * batch_x.size(0)

        # å®šæœŸåœ¨ç›®æ ‡åŸŸæµ‹è¯•é›†ä¸Šè¯„ä¼° RMSEï¼Œç”¨äºæ—©åœ
        if (epoch + 1) % eval_interval == 0 or epoch == target_epochs - 1:
            avg_loss = total_loss / len(loader.dataset)
            eval_res = evaluate_model(target_model, X_test, y_test.unsqueeze(1))
            rmse = eval_res['rmse']
            print(f"  Epoch [{epoch+1}/{target_epochs}] | Loss: {avg_loss:.4f} | Test RMSE: {rmse:.4f}")

            if rmse < best_rmse - min_delta:
                best_rmse = rmse
                epochs_no_improve = 0
                best_state = {k: v.cpu().clone() for k, v in target_model.state_dict().items()}
            else:
                epochs_no_improve += 1
                if early_stopping and epochs_no_improve >= patience:
                    print(f"â¹ï¸  æ—©åœè§¦å‘ï¼šè¿ç»­ {epochs_no_improve} æ¬¡è¯„ä¼° RMSE æœªæå‡ï¼Œåœæ­¢å¾®è°ƒã€‚")
                    break

    # æ¢å¤åˆ° RMSE æœ€ä¼˜çš„å‚æ•°
    if best_state is not None:
        target_model.load_state_dict(best_state)

    ft_time = time.time() - start_time
    final_res, infer_time = evaluate_model(target_model, X_test, y_test.unsqueeze(1), return_time=True)
    return final_res, ft_time, infer_time


# ========================== 5. ç»“æœä¿å­˜ä¸å¯è§†åŒ– ==========================
def create_results_root(source_epochs, target_epochs):
    """ç”Ÿæˆç»“æœæ ¹ç›®å½•ï¼ŒåŒ…å«å…³é”®å‚æ•°+æ—¶é—´æˆ³"""
    base_hidden = 64
    param_str = f"BLCAP_hid{base_hidden}_SrcEp{source_epochs}_TgtEp{target_epochs}"
    time_str = datetime.now().strftime('%Y%m%d_%H%M%S')
    root_dir = f"BLCAP_finetune_{param_str}_{time_str}"
    os.makedirs(root_dir, exist_ok=True)
    return root_dir, param_str


def save_prediction_csv(root_dir, target_key, eval_res,
                        source_epochs, target_epochs):
    """ä¿å­˜é¢„æµ‹ç»“æœ"""
    sorted_idx = np.argsort(eval_res['true_rul'])[::-1]
    sorted_true = eval_res['true_rul'][sorted_idx]
    sorted_pred = eval_res['pred_rul'][sorted_idx]
    abs_error = np.abs(sorted_true - sorted_pred)

    df = pd.DataFrame({
        'Dataset': [target_key] * len(sorted_true),
        'Model': ['BLCAP+FineTuning'] * len(sorted_true),
        'Source_Epochs': [source_epochs] * len(sorted_true),
        'Target_Epochs': [target_epochs] * len(sorted_true),
        'True_RUL': sorted_true,
        'Predicted_RUL': sorted_pred,
        'Absolute_Error': abs_error
    })

    save_path = os.path.join(
        root_dir,
        f"{target_key}_BLCAP_finetune_S{source_epochs}_T{target_epochs}_predictions.csv"
    )
    df.to_csv(save_path, index=False)
    return save_path


def save_performance_csv(root_dir, performance_list, is_init=False):
    """ä¿å­˜æ€§èƒ½æ±‡æ€»è¡¨"""
    df = pd.DataFrame(performance_list)
    save_path = os.path.join(root_dir, "BLCAP_finetune_performance.csv")
    if is_init:
        df.to_csv(save_path, index=False, mode='w')
    else:
        df.to_csv(save_path, index=False, mode='a', header=False)
    return save_path


def save_network_params_csv(root_dir, network_params_list):
    """ä¿å­˜ç½‘ç»œç»“æ„ä¸è®­ç»ƒè¶…å‚æ•°"""
    columns = [
        "Experiment_Name",
        "Model_Name",
        "Model_Type",
        "Input_Size",
        "Base_HiddenSize",
        "Batch_Size",
        "Learning_Rate",
        "Source_Epochs",
        "Target_Epochs",
        "PINN_Alpha",
        "Monotonic_Beta",
        "Loss_Function"
    ]
    df = pd.DataFrame(network_params_list, columns=columns)
    save_path = os.path.join(root_dir, "BLCAP_network_parameters.csv")
    df.to_csv(save_path, index=False, encoding="utf-8")
    print(f"ğŸ’¾ ç½‘ç»œå‚æ•°æ–‡ä»¶å·²ä¿å­˜: {save_path}")
    return save_path


def plot_error_curve(root_dir, target_key, eval_res,
                     source_epochs, target_epochs):
    """ç»˜åˆ¶è¯¯å·®æ›²çº¿"""
    sorted_idx = np.argsort(eval_res['true_rul'])[::-1]
    sorted_true = eval_res['true_rul'][sorted_idx]
    sorted_pred = eval_res['pred_rul'][sorted_idx]
    abs_error = np.abs(sorted_true - sorted_pred)

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)

    ax1.plot(sorted_true, label='True RUL', linewidth=2)
    ax1.plot(sorted_pred, label='Predicted RUL', linewidth=2, alpha=0.8)
    ax1.set_ylabel('Normalized RUL')
    ax1.set_title(f'{target_key} - BLCAP+FineTuning\n'
                  f'Source_Epochs={source_epochs}, Target_Epochs={target_epochs}')
    ax1.legend()
    ax1.grid(alpha=0.3)

    ax2.plot(abs_error, label='Absolute Error', linewidth=2)
    ax2.set_xlabel('Sample Index (sorted by True RUL desc)')
    ax2.set_ylabel('Absolute Error')
    ax2.legend()
    ax2.grid(alpha=0.3)

    plot_path = os.path.join(
        root_dir,
        f"{target_key}_BLCAP_finetune_S{source_epochs}_T{target_epochs}_error_curve.png"
    )
    plt.tight_layout()
    plt.savefig(plot_path, dpi=300)
    plt.close()
    return plot_path


# ========================== 6. ä¸»å®éªŒæµç¨‹ï¼šåªè·‘ BLCAP+FineTuning ==========================
def run_blcap_finetune_experiments(source_epochs, target_epochs):
    results_root, param_str = create_results_root(source_epochs, target_epochs)
    performance_list = []
    network_params_list = []

    print(f"ğŸš€ å¼€å§‹ BLCAP + Fine-Tuning å®éªŒ | æºåŸŸ epochs={source_epochs}, ç›®æ ‡åŸŸ epochs={target_epochs}")
    print(f"ğŸ“ ç»“æœç›®å½•: {results_root}")
    print(f"ğŸ“‹ é…ç½®æ‘˜è¦: {param_str}\n")

    for exp_idx, exp in enumerate(experiments, 1):
        exp_name = exp['name']
        source_keys = exp['source_keys']
        target_key = exp['target_key']
        print(f"{'=' * 80}")
        print(f"å®éªŒ {exp_idx}/6: {exp_name}")
        print(f"{'=' * 80}")

        # 1. æ•°æ®å‡†å¤‡
        print("\n1. æ•°æ®å‡†å¤‡")
        source_X, source_y = merge_source_datasets(source_keys)
        source_X_tensor = prepare_tensors(source_X, device)
        source_y_tensor = torch.tensor(source_y, dtype=torch.float32).to(device)

        target_data = load_single_dataset(target_key)
        target_data = add_time_feature(target_data)
        target_X = target_data.iloc[:, :-1].values
        target_y = target_data.iloc[:, -1].values
        X_ft, y_ft, X_test, y_test = split_target_data(target_X, target_y)

        ft_X_tensor = prepare_tensors(X_ft, device)
        ft_y_tensor = torch.tensor(y_ft, dtype=torch.float32).to(device)
        test_X_tensor = prepare_tensors(X_test, device)
        test_y_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)

        input_size = source_X.shape[1]

        # 2. æºåŸŸ BLCAP é¢„è®­ç»ƒ
        print("\n2. æºåŸŸ BLCAP é¢„è®­ç»ƒ")
        source_model = BaseModel(input_size, hidden_size=64).to(device)
        source_model, pretrain_time = pretrain_source_model(
            source_model, source_X_tensor, source_y_tensor, device, source_epochs
        )

        # 3. ç›®æ ‡åŸŸ BLCAP + Fine-Tuningï¼ˆå¸¦æ—©åœï¼‰
        print("\n3. ç›®æ ‡åŸŸ BLCAP + Fine-Tuning")
        target_model = BaseModel(input_size, hidden_size=64).to(device)
        final_res, ft_time, infer_time = train_fine_tuning(
            source_model, target_model,
            ft_X_tensor, ft_y_tensor,
            test_X_tensor, test_y_tensor,
            device, target_epochs
        )

        total_train_time = pretrain_time + ft_time

        # 4. ä¿å­˜ç»“æœ
        print(f"\nğŸ“Š å®éªŒ {exp_name} ç»“æœ | MAE: {final_res['mae']:.4f} | "
              f"RMSE: {final_res['rmse']:.4f} | R2: {final_res['r2']:.4f}")
        print(f"â±ï¸  é¢„è®­ç»ƒ: {pretrain_time:.2f}s | å¾®è°ƒ: {ft_time:.2f}s | æ€»è®­ç»ƒ: {total_train_time:.2f}s "
              f"| æ¨ç†: {infer_time:.4f}s")

        pred_path = save_prediction_csv(
            results_root, target_key, final_res,
            source_epochs, target_epochs
        )
        print(f"ğŸ’¾ é¢„æµ‹ç»“æœ CSV: {pred_path}")

        plot_path = plot_error_curve(
            results_root, target_key, final_res,
            source_epochs, target_epochs
        )
        print(f"ğŸ“ˆ è¯¯å·®æ›²çº¿ PNG: {plot_path}")

        # æ€§èƒ½è®°å½•
        performance = {
            'Experiment': exp_name,
            'Target_Dataset': target_key,
            'Model': 'BLCAP+FineTuning',
            'Source_Epochs': source_epochs,
            'Target_Epochs': target_epochs,
            'MAE': round(final_res['mae'], 4),
            'RMSE': round(final_res['rmse'], 4),
            'R2': round(final_res['r2'], 4),
            'Pretrain_Time(s)': round(pretrain_time, 2),
            'FT_Time(s)': round(ft_time, 2),
            'Train_Time_Total(s)': round(total_train_time, 2),
            'Infer_Time(s)': round(infer_time, 4),
            'Batch_Size': batch_size,
            'Device': str(device)
        }
        performance_list.append(performance)

        # ç½‘ç»œå‚æ•°è®°å½•
        net_params = [
            exp_name,
            'BLCAP+FineTuning',
            'BaseModel(BLCAP)',
            input_size,
            64,
            batch_size,
            learning_rate,
            source_epochs,
            target_epochs,
            alpha,
            beta,
            loss_fn.__class__.__name__
        ]
        network_params_list.append(net_params)

        # å†™å…¥æ€§èƒ½ CSVï¼ˆè¿½åŠ ï¼‰
        if exp_idx == 1:
            save_performance_csv(results_root, performance_list, is_init=True)
        else:
            save_performance_csv(results_root, performance_list[-1:], is_init=False)

        print(f"\n{'=' * 80}\n")

    # å†™å…¥ç½‘ç»œå‚æ•° CSV
    save_network_params_csv(results_root, network_params_list)

    print(f"ğŸ‰ BLCAP + Fine-Tuning å…¨éƒ¨ 6 ç»„å®éªŒå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœç›®å½•: {results_root}")
    print(f"ğŸ“„ æ€§èƒ½æ±‡æ€»è¡¨: {os.path.join(results_root, 'BLCAP_finetune_performance.csv')}")
    print(f"ğŸ“„ ç½‘ç»œå‚æ•°è¡¨: {os.path.join(results_root, 'BLCAP_network_parameters.csv')}")
    return results_root


# ========================== 7. ä¸»å…¥å£ ==========================
if __name__ == "__main__":
    start_time = time.time()
    results_dir = run_blcap_finetune_experiments(source_epochs, target_epochs)
    total_time = time.time() - start_time
    print(f"\nâ±ï¸  æ‰€æœ‰ BLCAP+FineTuning å®éªŒæ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
    print(f"ğŸ“  æœ€ç»ˆç»“æœç›®å½•: {results_dir}")

