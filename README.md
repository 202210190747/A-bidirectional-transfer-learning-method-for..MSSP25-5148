# A-bidirectional-transfer-learning-method-for..MSSP25-5148
Includes the code, public datasets, and robot milling data

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import time
from datetime import datetime

# -------------------------- 1. æ ¸å¿ƒå‚æ•°é…ç½® --------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

datasets = {
    'C1': 'filtered_data_normalized-C1.csv',
    'C4': 'filtered_data_normalized-C4.csv',
    'C6': 'filtered_data_normalized-C6.csv',
    'RC1': 'filtered_data_normalized-ROBOT-C1.csv',
    'RC2': 'filtered_data_normalized-ROBOT-C2.csv',
    'RC3': 'filtered_data_normalized-ROBOT-C3.csv'
}


experiments = [
    {'name': 'C1+C4+C6â†’RC1', 'source_keys': ['C1', 'C4', 'C6'], 'target_key': 'RC1'},
    {'name': 'C1+C4+C6â†’RC2', 'source_keys': ['C1', 'C4', 'C6'], 'target_key': 'RC2'},
    {'name': 'C1+C4+C6â†’RC3', 'source_keys': ['C1', 'C4', 'C6'], 'target_key': 'RC3'},
]

# å­¦ä¹ æ–¹æ³•åˆ—è¡¨ï¼ˆä¿æŒä¸å˜ï¼Œä¸åŸä»£ç ä¸€è‡´ï¼‰
methods = [
    'fine_tuning',          # è¿ç§»ï¼šå¾®è°ƒ
    'feature_extraction',   # è¿ç§»ï¼šç‰¹å¾æå–
    'adversarial',          # è¿ç§»ï¼šå¯¹æŠ—æ€§å­¦ä¹ 
    'tcn_bigru_attention',  # ä»…æºåŸŸï¼šTCN-BiGRU-Attention
    'cnn_lstm',             # ä»…æºåŸŸï¼šCNN-LSTM
    'transformer'           # ä»…æºåŸŸï¼šTransformer
]

# è®­ç»ƒå‚æ•°ï¼ˆä¿æŒä¸å˜ï¼Œä¸åŸä»£ç ä¸€è‡´ï¼‰ å‚æ•°å¯ä»¥ä½¿ç”¨optè¿›è¡Œä¼˜åŒ–
source_epochs = 25      # è¿™ä¸ªåœ°æ–¹çš„è®¾ç½®å¯ä»¥æŒ‰éœ€è®¾ç½®
target_epochs = 25       
batch_size = 64           
learning_rate = 0.001     # å¿«é€Ÿè·‘é€š
alpha = 0.01              # PINNç‰©ç†æŸå¤±æƒé‡
beta = 0.01               # å•è°ƒæ€§æŸå¤±æƒé‡
lambda_adv = 0.1          # å¯¹æŠ—æŸå¤±æƒé‡
loss_fn = nn.MSELoss()    # å›å½’æŸå¤±å‡½æ•°


# -------------------------- 2. æ•°æ®å¤„ç†å·¥å…·å‡½æ•° --------------------------
# ï¼ˆå®Œå…¨ä¸å˜ï¼Œä¸åŸä»£ç ä¸€è‡´ï¼‰
def load_single_dataset(dataset_key):
    try:
        data = pd.read_csv(datasets[dataset_key])
        if data.shape[1] < 2:
            raise ValueError(f"æ•°æ®é›†{dataset_key}æ ¼å¼é”™è¯¯ï¼Œéœ€è‡³å°‘1ç‰¹å¾+1æ ‡ç­¾")
        print(f"âœ… åŠ è½½æ•°æ®é›†: {datasets[dataset_key]} | æ ·æœ¬æ•°: {len(data)} | ç‰¹å¾æ•°: {data.shape[1]-1}")
        return data
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶: {datasets[dataset_key]}")
        exit()

def add_time_feature(data):
    if 'Time' not in data.columns:
        data.insert(0, 'Time', np.linspace(0, 1, len(data)))  # æ—¶é—´ç‰¹å¾å½’ä¸€åŒ–åˆ°0~1
    return data

def merge_source_datasets(source_keys):
    merged_X, merged_y = None, None
    for key in source_keys:
        data = load_single_dataset(key)
        data = add_time_feature(data)
        X = data.iloc[:, :-1].values  # ç‰¹å¾ï¼ˆå«æ—¶é—´ï¼‰ï¼š[æ ·æœ¬æ•°, ç‰¹å¾æ•°]
        y = data.iloc[:, -1].values   # æ ‡ç­¾ï¼ˆRULï¼‰ï¼š[æ ·æœ¬æ•°]
        if merged_X is None:
            merged_X, merged_y = X, y
        else:
            merged_X = np.concatenate([merged_X, X], axis=0)
            merged_y = np.concatenate([merged_y, y], axis=0)
    print(f"ğŸ“Š åˆå¹¶æºåŸŸ {source_keys} | æ€»æ ·æœ¬æ•°: {len(merged_X)} | ç‰¹å¾æ•°: {merged_X.shape[1]}")
    return merged_X, merged_y

def split_target_data(X_target, y_target):
    """ç›®æ ‡åŸŸåˆ’åˆ†ï¼šæµ‹è¯•é›†ï¼ˆæ¯éš”5é€‰1ï¼‰+ å¾®è°ƒé›†ï¼ˆå‰©ä½™æ¯éš”8é€‰1ï¼‰"""
    total_indices = np.arange(len(X_target))
    
    # æµ‹è¯•é›†ï¼ˆ~20%ï¼‰
    test_indices = total_indices[::5]
    X_test = X_target[test_indices]
    y_test = y_target[test_indices]
    
    # å¾®è°ƒé›†ï¼ˆä»å‰©ä½™æ ·æœ¬ä¸­é€‰ï¼‰
    remaining_indices = np.setdiff1d(total_indices, test_indices)
    if len(remaining_indices) < 8:  # é¿å…æ ·æœ¬æ•°ä¸è¶³å¯¼è‡´æ— æ•°æ®
        ft_indices = remaining_indices
    else:
        ft_indices = remaining_indices[::8]
    
    X_ft = X_target[ft_indices]
    y_ft = y_target[ft_indices]
    
    print(f"ğŸ¯ ç›®æ ‡åŸŸé‡‡æ · | å¾®è°ƒé›†: {len(X_ft)} æ ·æœ¬ | æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    return X_ft, y_ft, X_test, y_test

def prepare_tensors(data, device):
    """è½¬æ¢ä¸ºæ—¶åºæ¨¡å‹æ ‡å‡†æ ¼å¼ï¼š[æ ·æœ¬æ•°, seq_len=1, ç‰¹å¾æ•°]"""
    tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # æ–°å¢seq_lenç»´åº¦
    return tensor.to(device)  # æœ€ç»ˆå½¢çŠ¶ï¼š[batch, seq_len=1, features]


# -------------------------- 3. æ¨¡å‹å®šä¹‰ --------------------------
# ï¼ˆä»…BaseModelæ˜¾å¼æ·»åŠ hidden_sizeå‚æ•°ï¼Œå…¶ä½™ä¿æŒä¸å˜ï¼‰
class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, hidden_size, reduction_ratio=4):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size//reduction_ratio, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_size//reduction_ratio, hidden_size, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x: [batch, seq_len, hidden_size]
        b, l, c = x.size()
        y = self.avg_pool(x.permute(0,2,1)).view(b, c)  # é€šé“å…¨å±€æ± åŒ–
        y = self.fc(y).view(b, 1, c)                    # ç”Ÿæˆé€šé“æƒé‡
        return x * y.expand_as(x)                       # æ–½åŠ æ³¨æ„åŠ›æƒé‡

class BaseModel(nn.Module):
    """åŸºç¡€æ¨¡å‹ï¼ˆç”¨äºè¿ç§»å­¦ä¹ ï¼‰- æ˜¾å¼æ·»åŠ hidden_sizeå‚æ•°"""
    def __init__(self, input_size, hidden_size=64):  # æ˜¾å¼å®šä¹‰hidden_sizeï¼Œé»˜è®¤64
        super().__init__()
        self.hidden_size = hidden_size  # è®°å½•éšè—å±‚å¤§å°ï¼Œä¾¿äºåç»­å‚æ•°ä¿å­˜
        self.bilstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            bidirectional=True,
            batch_first=True
        )
        self.channel_attn = ChannelAttention(hidden_size*2)  # åŒå‘è¾“å‡ºç»´åº¦ç¿»å€
        self.fc = nn.Sequential(
            nn.Linear(hidden_size*2, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 1),
            nn.Sigmoid()  # RULå½’ä¸€åŒ–åˆ°0~1
        )
        self.time_idx = 0  # æ—¶é—´ç‰¹å¾ç´¢å¼•

    def forward(self, x):
        # x: [batch, seq_len, features]
        x, _ = self.bilstm(x)  # [batch, seq_len, hidden_size*2]
        x = self.channel_attn(x)
        return self.fc(x[:, -1, :])  # å–æœ€åæ—¶é—´æ­¥è¾“å‡º

    def physics_loss(self, y_pred, x):
        """PINNç‰©ç†çº¦æŸï¼šRULéšæ—¶é—´é€’å‡"""
        t = x[:, :, self.time_idx].squeeze()  # æå–æ—¶é—´ç‰¹å¾
        return torch.mean(torch.relu(y_pred.squeeze() - (1 - t)))

    def monotonicity_loss(self, y_pred, x):
        """å•è°ƒæ€§çº¦æŸï¼šRULéšæ—¶é—´å•è°ƒé€’å‡"""
        t = x[:, :, self.time_idx].squeeze()
        sorted_idx = torch.argsort(t)
        sorted_pred = y_pred[sorted_idx]
        diffs = sorted_pred[1:] - sorted_pred[:-1]  # åå€¼-å‰å€¼â‰¤0
        return torch.mean(torch.relu(diffs))

    def feature_extractor(self, x):
        """ç‰¹å¾æå–å™¨ï¼ˆç”¨äºè¿ç§»å­¦ä¹ ï¼‰"""
        x, _ = self.bilstm(x)
        return self.channel_attn(x)

class TCNLayer(nn.Module):
    """TCNå±‚ï¼ˆé€‚é…çŸ­åºåˆ—ï¼‰"""
    def __init__(self, in_channels, out_channels, kernel_size=2, dilation=1):
        super().__init__()
        self.conv = nn.Conv1d(
            in_channels, out_channels,
            kernel_size=kernel_size,
            padding=0,
            dilation=dilation
        )
        self.batch_norm = nn.BatchNorm1d(out_channels)
        self.relu = nn.ReLU()
        self.residual = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None

    def forward(self, x):
        # x: [batch, in_channels, seq_len]
        residual = x
        
        # è‹¥seq_len < æ ¸å¤§å°ï¼Œç”¨paddingè¡¥å…¨
        if x.size(2) < self.conv.kernel_size[0]:
            pad_size = self.conv.kernel_size[0] - x.size(2)
            x = nn.functional.pad(x, (0, pad_size))  # å³ä¾§è¡¥0
        
        x = self.conv(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        
        # æ®‹å·®è¿æ¥ç»´åº¦å¯¹é½
        if self.residual is not None:
            residual = self.residual(residual)
        if x.size(2) != residual.size(2):
            residual = nn.functional.adaptive_avg_pool1d(residual, x.size(2))
        
        return x + residual

class TCNBiGRUAttention(nn.Module):
    """TCN-BiGRU-Attentionæ¨¡å‹"""
    def __init__(self, input_size, tcn_channels=[32, 64], gru_hidden=64, num_heads=2):
        super().__init__()
        self.tcn_input_proj = nn.Linear(input_size, tcn_channels[0])  # ç‰¹å¾æŠ•å½±
        tcn_layers = []
        in_channels = tcn_channels[0]
        for out_channels in tcn_channels[1:]:
            tcn_layers.append(TCNLayer(in_channels, out_channels))
            in_channels = out_channels
        self.tcn = nn.Sequential(*tcn_layers)
        
        self.bigru = nn.GRU(
            input_size=tcn_channels[-1],
            hidden_size=gru_hidden,
            bidirectional=True,
            batch_first=True
        )
        
        self.attention = nn.MultiheadAttention(
            embed_dim=gru_hidden*2,
            num_heads=num_heads,
            batch_first=True
        )
        
        self.fc = nn.Sequential(
            nn.Linear(gru_hidden*2, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x: [batch, seq_len, features]
        x_proj = self.tcn_input_proj(x)  # [batch, seq_len, tcn_channels[0]]
        x_tcn = x_proj.permute(0, 2, 1)  # [batch, channels, seq_len]
        x_tcn = self.tcn(x_tcn)          # [batch, tcn_channels[-1], seq_len]
        
        x_gru = x_tcn.permute(0, 2, 1)   # [batch, seq_len, tcn_channels[-1]]
        x_gru, _ = self.bigru(x_gru)     # [batch, seq_len, gru_hidden*2]
        
        attn_output, _ = self.attention(x_gru, x_gru, x_gru)  # è‡ªæ³¨æ„åŠ›
        x_out = attn_output[:, -1, :]    # å–æœ€åæ—¶é—´æ­¥
        
        return self.fc(x_out)  # [batch, 1]

class CNNLSTM(nn.Module):
    """CNN-LSTMæ¨¡å‹ï¼ˆä¿®å¤æ± åŒ–é—®é¢˜ï¼‰"""
    def __init__(self, input_size, cnn_filters=[32, 64], lstm_hidden=64):
        super().__init__()
        # CNNéƒ¨åˆ†ï¼ˆç§»é™¤æ± åŒ–å±‚ï¼‰
        cnn_layers = []
        in_channels = input_size
        for out_channels in cnn_filters:
            cnn_layers.extend([
                nn.Conv1d(in_channels, out_channels, kernel_size=1),  # 1x1å·ç§¯
                nn.BatchNorm1d(out_channels),
                nn.ReLU()
            ])
            in_channels = out_channels
        self.cnn = nn.Sequential(*cnn_layers)
        
        self.lstm = nn.LSTM(
            input_size=cnn_filters[-1],
            hidden_size=lstm_hidden,
            num_layers=1,
            batch_first=True,
            dropout=0.2
        )
        
        self.fc = nn.Sequential(
            nn.Linear(lstm_hidden, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x: [batch, seq_len, features]
        x_cnn = x.permute(0, 2, 1)  # [batch, features, seq_len]
        x_cnn = self.cnn(x_cnn)     # [batch, cnn_filters[-1], seq_len]
        
        x_lstm = x_cnn.permute(0, 2, 1)  # [batch, seq_len, cnn_filters[-1]]
        x_lstm, _ = self.lstm(x_lstm)    # [batch, seq_len, lstm_hidden]
        
        x_out = x_lstm[:, -1, :]         # å–æœ€åæ—¶é—´æ­¥
        return self.fc(x_out)  # [batch, 1]

class TransformerModel(nn.Module):
    """Transformeræ¨¡å‹"""
    def __init__(self, input_size, d_model=64, nhead=2, num_layers=1):
        super().__init__()
        self.proj = nn.Linear(input_size, d_model)  # ç‰¹å¾æŠ•å½±
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=128,
            dropout=0.2,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.fc = nn.Sequential(
            nn.Linear(d_model, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x: [batch, seq_len, features]
        x_proj = self.proj(x)  # [batch, seq_len, d_model]
        x_enc = self.transformer_encoder(x_proj)  # [batch, seq_len, d_model]
        x_out = x_enc[:, -1, :]  # å–æœ€åæ—¶é—´æ­¥
        return self.fc(x_out)  # [batch, 1]


# -------------------------- 4. è¿ç§»å­¦ä¹ ç»„ä»¶ä¸è®­ç»ƒæ–¹æ³• --------------------------
# ï¼ˆå®Œå…¨ä¸å˜ï¼Œä¸åŸä»£ç ä¸€è‡´ï¼‰
class GradientReversalLayer(nn.Module):
    """æ¢¯åº¦åè½¬å±‚ï¼ˆå¯¹æŠ—æ€§è¿ç§»ï¼‰"""
    def __init__(self, lambda_=1.0):
        super().__init__()
        self.lambda_ = lambda_

    def forward(self, x):
        return x

    def backward(self, grad_output):
        return grad_output * (-self.lambda_)

class DomainClassifier(nn.Module):
    """åŸŸåˆ†ç±»å™¨ï¼ˆå¯¹æŠ—æ€§è¿ç§»ï¼‰"""
    def __init__(self, input_dim=128):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x_avg = torch.mean(x, dim=1)  # [batch, features]
        return self.fc(x_avg)

def evaluate_model(model, X_tensor, y_tensor):
    """æ¨¡å‹è¯„ä¼°å‡½æ•°"""
    model.eval()
    with torch.no_grad():
        y_pred = model(X_tensor).cpu().numpy().flatten()
        y_true = y_tensor.cpu().numpy().flatten()
    
    return {
        'true_rul': y_true,
        'pred_rul': y_pred,
        'mae': mean_absolute_error(y_true, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
        'r2': r2_score(y_true, y_pred)
    }

def train_source_only_model(model, X_train, y_train, X_test, y_test, device, model_name):
    """ä»…æºåŸŸè®­ç»ƒæ¨¡å‹"""
    print(f"\nğŸ“Œ {model_name} æºåŸŸè®­ç»ƒï¼ˆæ— ç›®æ ‡åŸŸå¾®è°ƒï¼‰")
    dataset = TensorDataset(X_train, y_train.unsqueeze(1))
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    
    start_time = time.time()
    for epoch in range(source_epochs):
        model.train()
        total_loss = 0.0
        
        for batch_x, batch_y in loader:
            optimizer.zero_grad()
            y_pred = model(batch_x)
            loss = loss_fn(y_pred, batch_y)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item() * batch_x.size(0)
        
        if (epoch + 1) % 5 == 0:
            avg_loss = total_loss / len(loader.dataset)
            eval_res = evaluate_model(model, X_test, y_test.unsqueeze(1))
            print(f"  Epoch [{epoch+1}/{source_epochs}] | Loss: {avg_loss:.4f} | Test RMSE: {eval_res['rmse']:.4f}")
    
    train_time = time.time() - start_time
    final_res = evaluate_model(model, X_test, y_test.unsqueeze(1))
    print(f"ğŸ“Œ {model_name} è®­ç»ƒå®Œæˆ | è€—æ—¶: {train_time:.2f} ç§’")
    return final_res, train_time

def pretrain_source_model(model, X_train, y_train, device):
    """æºåŸŸé¢„è®­ç»ƒï¼ˆè¿ç§»å­¦ä¹ å…±ç”¨ï¼‰"""
    dataset = TensorDataset(X_train, y_train.unsqueeze(1))
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    
    start_time = time.time()
    for epoch in range(source_epochs):
        model.train()
        total_loss = 0.0
        
        for batch_x, batch_y in loader:
            optimizer.zero_grad()
            y_pred = model(batch_x)
            
            data_loss = loss_fn(y_pred, batch_y)
            pde_loss = model.physics_loss(y_pred, batch_x)
            mono_loss = model.monotonicity_loss(y_pred, batch_x)
            loss = data_loss + alpha*pde_loss + beta*mono_loss
            
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item() * batch_x.size(0)
        
        if (epoch + 1) % 5 == 0:
            avg_loss = total_loss / len(loader.dataset)
            print(f"  æºåŸŸé¢„è®­ç»ƒ Epoch [{epoch+1}/{source_epochs}] | Loss: {avg_loss:.4f}")
    
    pretrain_time = time.time() - start_time
    print(f"ğŸ“Œ æºåŸŸé¢„è®­ç»ƒå®Œæˆ | è€—æ—¶: {pretrain_time:.2f} ç§’")
    return model, pretrain_time

# è¿ç§»å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼ˆä¿æŒä¸å˜ï¼‰
def train_fine_tuning(source_model, target_model, X_ft, y_ft, X_test, y_test, device):
    print("\nğŸ“Œ å¾®è°ƒè¿ç§»å­¦ä¹ è®­ç»ƒ")
    target_model.load_state_dict(source_model.state_dict())
    dataset = TensorDataset(X_ft, y_ft.unsqueeze(1))
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = optim.AdamW(target_model.parameters(), lr=learning_rate/10, weight_decay=1e-4)
    
    start_time = time.time()
    for epoch in range(target_epochs):
        target_model.train()
        total_loss = 0.0
        
        for batch_x, batch_y in loader:
            optimizer.zero_grad()
            y_pred = target_model(batch_x)
            data_loss = loss_fn(y_pred, batch_y)
            pde_loss = target_model.physics_loss(y_pred, batch_x)
            mono_loss = target_model.monotonicity_loss(y_pred, batch_x)
            loss = data_loss + alpha*pde_loss + beta*mono_loss
            
            loss.backward()
            nn.utils.clip_grad_norm_(target_model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item() * batch_x.size(0)
        
        if (epoch + 1) % 5 == 0:
            avg_loss = total_loss / len(loader.dataset)
            eval_res = evaluate_model(target_model, X_test, y_test.unsqueeze(1))
            print(f"  Epoch [{epoch+1}/{target_epochs}] | Loss: {avg_loss:.4f} | Test RMSE: {eval_res['rmse']:.4f}")
    
    ft_time = time.time() - start_time
    final_res = evaluate_model(target_model, X_test, y_test.unsqueeze(1))
    return final_res, ft_time

def train_feature_extraction(source_model, target_model, X_ft, y_ft, X_test, y_test, device):
    print("\nğŸ“Œ ç‰¹å¾æå–è¿ç§»å­¦ä¹ è®­ç»ƒ")
    target_model.load_state_dict(source_model.state_dict())
    # å†»ç»“ç‰¹å¾å±‚
    for param in target_model.bilstm.parameters():
        param.requires_grad = False
    for param in target_model.channel_attn.parameters():
        param.requires_grad = False
    
    dataset = TensorDataset(X_ft, y_ft.unsqueeze(1))
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = optim.AdamW(target_model.fc.parameters(), lr=learning_rate/5, weight_decay=1e-4)
    
    start_time = time.time()
    for epoch in range(target_epochs):
        target_model.train()
        total_loss = 0.0
        
        for batch_x, batch_y in loader:
            optimizer.zero_grad()
            y_pred = target_model(batch_x)
            data_loss = loss_fn(y_pred, batch_y)
            pde_loss = target_model.physics_loss(y_pred, batch_x)
            mono_loss = target_model.monotonicity_loss(y_pred, batch_x)
            loss = data_loss + alpha*pde_loss + beta*mono_loss
            
            loss.backward()
            optimizer.step()
            total_loss += loss.item() * batch_x.size(0)
        
        if (epoch + 1) % 5 == 0:
            avg_loss = total_loss / len(loader.dataset)
            eval_res = evaluate_model(target_model, X_test, y_test.unsqueeze(1))
            print(f"  Epoch [{epoch+1}/{target_epochs}] | Loss: {avg_loss:.4f} | Test RMSE: {eval_res['rmse']:.4f}")
    
    ft_time = time.time() - start_time
    final_res = evaluate_model(target_model, X_test, y_test.unsqueeze(1))
    return final_res, ft_time

def train_mmd_transfer(source_model, target_model, X_source, y_source, X_ft, y_ft, X_test, y_test, device):
    print("\nğŸ“Œ MMDè¿ç§»å­¦ä¹ è®­ç»ƒ")
    target_model.load_state_dict(source_model.state_dict())
    source_dataset = TensorDataset(X_source, y_source.unsqueeze(1))
    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)
    target_dataset = TensorDataset(X_ft, y_ft.unsqueeze(1))
    target_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True)
    
    optimizer = optim.AdamW(target_model.parameters(), lr=learning_rate/10, weight_decay=1e-4)
    start_time = time.time()
    target_iter = iter(target_loader)
    
    for epoch in range(target_epochs):
        target_model.train()
        total_loss = 0.0
        
        for batch_x, batch_y in source_loader:
            try:
                target_x, _ = next(target_iter)
            except StopIteration:
                target_iter = iter(target_loader)
                target_x, _ = next(target_iter)
            
            min_size = min(batch_x.size(0), target_x.size(0))
            batch_x, batch_y = batch_x[:min_size], batch_y[:min_size]
            target_x = target_x[:min_size]
            
            optimizer.zero_grad()
            y_pred = target_model(batch_x)
            
            data_loss = loss_fn(y_pred, batch_y)
            pde_loss = target_model.physics_loss(y_pred, batch_x)
            mono_loss = target_model.monotonicity_loss(y_pred, batch_x)
            
            source_feat = target_model.feature_extractor(batch_x)
            target_feat = target_model.feature_extractor(target_x)
            mmd = mmd_loss(source_feat, target_feat)
            
            loss = data_loss + alpha*pde_loss + beta*mono_loss + gamma*mmd
            loss.backward()
            nn.utils.clip_grad_norm_(target_model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item() * batch_x.size(0)
        
        if (epoch + 1) % 5 == 0:
            eval_res = evaluate_model(target_model, X_test, y_test.unsqueeze(1))
            print(f"  Epoch [{epoch+1}/{target_epochs}] | MMD: {mmd.item():.4f} | Test RMSE: {eval_res['rmse']:.4f}")
    
    ft_time = time.time() - start_time
    final_res = evaluate_model(target_model, X_test, y_test.unsqueeze(1))
    return final_res, ft_time

def train_adversarial(source_model, target_model, X_source, y_source, X_ft, y_ft, X_test, y_test, device):
    print("\nğŸ“Œ å¯¹æŠ—æ€§è¿ç§»å­¦ä¹ è®­ç»ƒ")
    target_model.load_state_dict(source_model.state_dict())
    domain_clf = DomainClassifier(input_dim=128).to(device)
    grl = GradientReversalLayer(lambda_=lambda_adv)
    
    source_dataset = TensorDataset(X_source, y_source.unsqueeze(1))
    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)
    target_dataset = TensorDataset(X_ft, y_ft.unsqueeze(1))
    target_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True)
    
    feat_optimizer = optim.AdamW(target_model.parameters(), lr=learning_rate/10, weight_decay=1e-4)
    clf_optimizer = optim.AdamW(domain_clf.parameters(), lr=learning_rate/10, weight_decay=1e-4)
    domain_criterion = nn.BCELoss()
    
    start_time = time.time()
    target_iter = iter(target_loader)
    
    for epoch in range(target_epochs):
        target_model.train()
        domain_clf.train()
        
        for batch_x, batch_y in source_loader:
            try:
                target_x, _ = next(target_iter)
            except StopIteration:
                target_iter = iter(target_loader)
                target_x, _ = next(target_iter)
            
            min_size = min(batch_x.size(0), target_x.size(0))
            batch_x, batch_y = batch_x[:min_size], batch_y[:min_size]
            target_x = target_x[:min_size]
            
            source_labels = torch.zeros(min_size, 1).to(device)
            target_labels = torch.ones(min_size, 1).to(device)
            
            # è®­ç»ƒåŸŸåˆ†ç±»å™¨
            clf_optimizer.zero_grad()
            source_feat = target_model.feature_extractor(batch_x)
            target_feat = target_model.feature_extractor(target_x)
            
            source_pred = domain_clf(grl(source_feat.detach()))
            target_pred = domain_clf(grl(target_feat.detach()))
            clf_loss = domain_criterion(source_pred, source_labels) + domain_criterion(target_pred, target_labels)
            clf_loss.backward()
            clf_optimizer.step()
            
            # è®­ç»ƒç‰¹å¾æå–å™¨
            feat_optimizer.zero_grad()
            y_pred = target_model(batch_x)
            
            data_loss = loss_fn(y_pred, batch_y)
            pde_loss = target_model.physics_loss(y_pred, batch_x)
            mono_loss = target_model.monotonicity_loss(y_pred, batch_x)
            
            source_pred = domain_clf(grl(source_feat))
            target_pred = domain_clf(grl(target_feat))
            adv_loss = domain_criterion(source_pred, target_labels) + domain_criterion(target_pred, source_labels)
            
            loss = data_loss + alpha*pde_loss + beta*mono_loss + lambda_adv*adv_loss
            loss.backward()
            nn.utils.clip_grad_norm_(target_model.parameters(), 1.0)
            feat_optimizer.step()
        
        if (epoch + 1) % 5 == 0:
            eval_res = evaluate_model(target_model, X_test, y_test.unsqueeze(1))
            print(f"  Epoch [{epoch+1}/{target_epochs}] | Adv Loss: {adv_loss.item():.4f} | Test RMSE: {eval_res['rmse']:.4f}")
    
    ft_time = time.time() - start_time
    final_res = evaluate_model(target_model, X_test, y_test.unsqueeze(1))
    return final_res, ft_time


# -------------------------- 5. ç»“æœä¿å­˜ä¸å¯è§†åŒ– --------------------------
# ï¼ˆæ–°å¢ï¼š1. æ–‡ä»¶å¤¹åç§°å«å‚æ•°ï¼›2. ç½‘ç»œå‚æ•°CSVä¿å­˜å‡½æ•°ï¼‰
def create_results_root():
    """ä¿®æ”¹ï¼šç”ŸæˆåŒ…å«å…³é”®ç½‘ç»œå‚æ•°çš„ç»“æœæ–‡ä»¶å¤¹åç§°"""
    # 1. å®šä¹‰æ‰€æœ‰æ¨¡å‹çš„å…³é”®ç»“æ„å‚æ•°ï¼ˆä¸æ¨¡å‹__init__é»˜è®¤å€¼å¯¹é½ï¼‰
    base_hidden = 64          # BaseModelçš„LSTMéšè—å±‚å¤§å°
    tcn_channels = [32, 64]   # TCN-BiGRU-Attentionçš„TCNé€šé“æ•°
    gru_hidden = 64           # TCN-BiGRU-Attentionçš„GRUéšè—å±‚å¤§å°
    cnn_filters = [32, 64]    # CNN-LSTMçš„CNNæ»¤æ³¢å™¨æ•°
    trans_d_model = 64        # Transformerçš„d_model
    
    # 2. æ ¼å¼åŒ–å‚æ•°å­—ç¬¦ä¸²ï¼ˆé¿å…ç‰¹æ®Šå­—ç¬¦ï¼Œç¡®ä¿æ–‡ä»¶å¤¹ååˆæ³•ï¼‰
    param_str = (
        f"hid{base_hidden}_"
        f"tcn{'-'.join(map(str, tcn_channels))}_"
        f"gru{gru_hidden}_"
        f"cnn{'-'.join(map(str, cnn_filters))}_"
        f"trans{trans_d_model}"
    )
    
    # 3. ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶å¤¹åï¼šå‰ç¼€_å‚æ•°_æ—¶é—´æˆ³
    time_str = datetime.now().strftime('%Y%m%d_%H%M%S')
    root_dir = f"transfer_comparison_{param_str}_{time_str}"
    os.makedirs(root_dir, exist_ok=True)
    
    return root_dir, param_str  # è¿”å›å‚æ•°å­—ç¬¦ä¸²ï¼Œä¾¿äºåç»­æ—¥å¿—æ‰“å°

def save_prediction_csv(root_dir, target_key, method, eval_res):
    """ï¼ˆä¿æŒä¸å˜ï¼‰ä¿å­˜é¢„æµ‹ç»“æœCSV"""
    sorted_idx = np.argsort(eval_res['true_rul'])[::-1]
    sorted_true = eval_res['true_rul'][sorted_idx]
    sorted_pred = eval_res['pred_rul'][sorted_idx]
    abs_error = np.abs(sorted_true - sorted_pred)
    
    model_name = method.replace('_', ' ').title()
    df = pd.DataFrame({
        'Dataset': [target_key] * len(sorted_true),
        'Model': [model_name] * len(sorted_true),
        'True_RUL': sorted_true,
        'Predicted_RUL': sorted_pred,
        'Absolute_Error': abs_error
    })
    
    save_path = os.path.join(root_dir, f"{target_key}_{method}_predictions.csv")
    df.to_csv(save_path, index=False)
    return save_path

def save_performance_csv(root_dir, performance_list, is_init=False):
    """ï¼ˆä¿æŒä¸å˜ï¼‰ä¿å­˜æ€§èƒ½æ±‡æ€»è¡¨"""
    df = pd.DataFrame(performance_list)
    save_path = os.path.join(root_dir, "all_models_performance.csv")
    
    if is_init:
        df.to_csv(save_path, index=False, mode='w')
    else:
        df.to_csv(save_path, index=False, mode='a', header=False)
    return save_path

def save_network_params_csv(root_dir, network_params_list):
    """æ–°å¢ï¼šä¿å­˜ç½‘ç»œæ¨¡å‹ç»“æ„ä¸è®­ç»ƒå‚æ•°åˆ°CSV"""
    # å®šä¹‰CSVåˆ—åï¼ˆè¦†ç›–æ‰€æœ‰æ¨¡å‹çš„å‚æ•°ç»´åº¦ï¼‰
    columns = [
        "Experiment_Name",       # å®éªŒåç§°ï¼ˆå¦‚RC1+RC2+RC3â†’C1ï¼‰
        "Model_Method",          # æ¨¡å‹æ–¹æ³•ï¼ˆå¦‚fine_tuningã€tcn_bigru_attentionï¼‰
        "Model_Type",            # æ¨¡å‹ç±»å‹ï¼ˆå¦‚BaseModelã€TCNBiGRUAttentionï¼‰
        "Input_Size",            # è¾“å…¥ç‰¹å¾æ•°ï¼ˆåŠ¨æ€è·å–ï¼‰
        # BaseModelå‚æ•°
        "BaseModel_HiddenSize",  # BaseModelçš„LSTMéšè—å±‚å¤§å°
        # TCN-BiGRU-Attentionå‚æ•°
        "TCN_Channels",          # TCNé€šé“æ•°ï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚"32,64"ï¼‰
        "GRU_Hidden",            # GRUéšè—å±‚å¤§å°
        "GRU_NumHeads",          # æ³¨æ„åŠ›å¤´æ•°
        # CNN-LSTMå‚æ•°
        "CNN_Filters",           # CNNæ»¤æ³¢å™¨æ•°ï¼ˆé€—å·åˆ†éš”ï¼Œå¦‚"32,64"ï¼‰
        "LSTM_Hidden",           # LSTMéšè—å±‚å¤§å°
        # Transformerå‚æ•°
        "Trans_DModel",          # Transformerçš„d_model
        "Trans_NHead",           # Transformeræ³¨æ„åŠ›å¤´æ•°
        "Trans_NLayers",         # Transformerç¼–ç å™¨å±‚æ•°
        # è®­ç»ƒå‚æ•°
        "Batch_Size",            # æ‰¹æ¬¡å¤§å°
        "Learning_Rate",         # å­¦ä¹ ç‡
        "Source_Epochs",         # æºåŸŸè®­ç»ƒè½®æ¬¡
        "Target_Epochs",         # ç›®æ ‡åŸŸè®­ç»ƒè½®æ¬¡
        "PINN_Alpha",            # PINNç‰©ç†æŸå¤±æƒé‡
        "Monotonic_Beta",        # å•è°ƒæ€§æŸå¤±æƒé‡
        "Adv_Lambda",            # å¯¹æŠ—æŸå¤±æƒé‡
        "Loss_Function"          # æŸå¤±å‡½æ•°ç±»å‹
    ]
    
    # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
    df = pd.DataFrame(network_params_list, columns=columns)
    save_path = os.path.join(root_dir, "network_model_parameters.csv")
    df.to_csv(save_path, index=False, encoding="utf-8")
    print(f"ğŸ’¾ ç½‘ç»œå‚æ•°æ–‡ä»¶å·²ä¿å­˜: {save_path}")
    return save_path

def plot_error_curve(root_dir, target_key, method, eval_res):
    """ï¼ˆä¿æŒä¸å˜ï¼‰ç»˜åˆ¶è¯¯å·®æ›²çº¿"""
    sorted_idx = np.argsort(eval_res['true_rul'])[::-1]
    sorted_true = eval_res['true_rul'][sorted_idx]
    sorted_pred = eval_res['pred_rul'][sorted_idx]
    abs_error = np.abs(sorted_true - sorted_pred)
    
    model_name = method.replace('_', ' ').title()
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
    
    ax1.plot(sorted_true, label='True RUL', color='#2E86AB', linewidth=2.5)
    ax1.plot(sorted_pred, label='Predicted RUL', color='#A23B72', linewidth=2, alpha=0.8)
    ax1.set_ylabel('Normalized RUL', fontsize=12)
    ax1.set_title(f'{target_key} - {model_name}\nPrediction vs True RUL', fontsize=14)
    ax1.legend()
    ax1.grid(alpha=0.3)
    
    ax2.plot(abs_error, color='#F18F01', linewidth=2, label='Absolute Error')
    ax2.set_xlabel('Sample Index (Sorted by True RUL Descending)', fontsize=12)
    ax2.set_ylabel('Absolute Error', fontsize=12)
    ax2.legend()
    ax2.grid(alpha=0.3)
    
    plot_path = os.path.join(root_dir, f"{target_key}_{method}_error_curve.png")
    plt.tight_layout()
    plt.savefig(plot_path, dpi=300)
    plt.close()
    return plot_path


# -------------------------- 6. ä¸»å®éªŒæµç¨‹ --------------------------
# ï¼ˆä¿®æ”¹ï¼šé›†æˆç½‘ç»œå‚æ•°æ”¶é›†ä¸CSVä¿å­˜ï¼‰
def run_all_experiments():
    # ä¿®æ”¹ï¼šè·å–å¸¦å‚æ•°çš„ç»“æœæ–‡ä»¶å¤¹è·¯å¾„
    results_root, param_str = create_results_root()
    performance_list = []
    # æ–°å¢ï¼šåˆå§‹åŒ–ç½‘ç»œå‚æ•°åˆ—è¡¨ï¼ˆç”¨äºä¿å­˜åˆ°CSVï¼‰
    network_params_list = []
    
    print(f"ğŸš€ å¼€å§‹è¿ç§»å­¦ä¹ ä¸å¯¹æ¯”æ¨¡å‹å®éªŒ | ç»“æœç›®å½•: {results_root}\n")
    print(f"ğŸ“‹ å…³é”®ç½‘ç»œå‚æ•°é…ç½®: {param_str}\n")
    
    for exp_idx, exp in enumerate(experiments, 1):
        exp_name = exp['name']
        source_keys = exp['source_keys']
        target_key = exp['target_key']
        print(f"{'='*80}")
        print(f"å®éªŒ {exp_idx}/6: {exp_name}")
        print(f"{'='*80}")
        
        # 1. æ•°æ®å‡†å¤‡ï¼ˆä¿æŒä¸å˜ï¼‰
        print("\n1. æ•°æ®å‡†å¤‡")
        source_X, source_y = merge_source_datasets(source_keys)
        source_X_tensor = prepare_tensors(source_X, device)
        source_y_tensor = torch.tensor(source_y, dtype=torch.float32).to(device)
        
        target_data = load_single_dataset(target_key)
        target_data = add_time_feature(target_data)
        target_X = target_data.iloc[:, :-1].values
        target_y = target_data.iloc[:, -1].values
        X_ft, y_ft, X_test, y_test = split_target_data(target_X, target_y)
        
        ft_X_tensor = prepare_tensors(X_ft, device)
        ft_y_tensor = torch.tensor(y_ft, dtype=torch.float32).to(device)
        test_X_tensor = prepare_tensors(X_test, device)
        test_y_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)
        
        # 2. æºåŸŸé¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ˜¾å¼ä¼ å…¥å›ºå®šå‚æ•°ï¼‰
        input_size = source_X.shape[1]  # åŠ¨æ€è·å–è¾“å…¥ç‰¹å¾æ•°
        # å®šä¹‰æ‰€æœ‰æ¨¡å‹çš„å›ºå®šç»“æ„å‚æ•°ï¼ˆä¸æ¨¡å‹é»˜è®¤å€¼ä¸€è‡´ï¼‰
        fixed_params = {
            "base_hidden": 64,
            "tcn_chs": [32, 64],
            "gru_hid": 64,
            "gru_heads": 2,
            "cnn_filts": [32, 64],
            "lstm_hid": 64,
            "trans_d": 64,
            "trans_head": 2,
            "trans_layers": 1
        }
        
        # æ˜¾å¼ä¼ å…¥hidden_sizeï¼Œç¡®ä¿å‚æ•°å¯è¿½æº¯
        source_model = BaseModel(input_size, hidden_size=fixed_params["base_hidden"]).to(device)
        source_model, pretrain_time = pretrain_source_model(
            source_model, source_X_tensor, source_y_tensor, device
        )
        
        # 3. éå†æ‰€æœ‰æ–¹æ³•ï¼ˆæ–°å¢å‚æ•°æ”¶é›†é€»è¾‘ï¼‰
        for method_idx, method in enumerate(methods, 1):
            print(f"\n{'='*60}")
            print(f"æ–¹æ³• {method_idx}/{len(methods)}: {method.replace('_', ' ').title()}")
            print(f"{'='*60}")
            
            # æ–°å¢ï¼šåˆå§‹åŒ–å½“å‰æ–¹æ³•çš„å‚æ•°å­—å…¸ï¼ˆç”¨äºåç»­CSVä¿å­˜ï¼‰
            current_params = {
                "Experiment_Name": exp_name,
                "Model_Method": method,
                "Input_Size": input_size,
                # BaseModelå‚æ•°
                "BaseModel_HiddenSize": fixed_params["base_hidden"],
                # TCN-BiGRU-Attentionå‚æ•°ï¼ˆæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼‰
                "TCN_Channels": ",".join(map(str, fixed_params["tcn_chs"])),
                "GRU_Hidden": fixed_params["gru_hid"],
                "GRU_NumHeads": fixed_params["gru_heads"],
                # CNN-LSTMå‚æ•°ï¼ˆæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼‰
                "CNN_Filters": ",".join(map(str, fixed_params["cnn_filts"])),
                "LSTM_Hidden": fixed_params["lstm_hid"],
                # Transformerå‚æ•°
                "Trans_DModel": fixed_params["trans_d"],
                "Trans_NHead": fixed_params["trans_head"],
                "Trans_NLayers": fixed_params["trans_layers"],
                # è®­ç»ƒå‚æ•°ï¼ˆç›´æ¥ä»å…¨å±€å˜é‡è¯»å–ï¼‰
                "Batch_Size": batch_size,
                "Learning_Rate": learning_rate,
                "Source_Epochs": source_epochs,
                "Target_Epochs": target_epochs,
                "PINN_Alpha": alpha,
                "Monotonic_Beta": beta,
                "Adv_Lambda": lambda_adv,
                "Loss_Function": loss_fn.__class__.__name__  # è·å–æŸå¤±å‡½æ•°ç±»åï¼ˆå¦‚MSELossï¼‰
            }
            
            # è¿ç§»å­¦ä¹ æ–¹æ³•ï¼ˆä½¿ç”¨BaseModelï¼‰
            if method in ['fine_tuning', 'feature_extraction', 'adversarial']:
                current_params["Model_Type"] = "BaseModel"  # æ ‡è®°æ¨¡å‹ç±»å‹
                target_model = BaseModel(input_size, hidden_size=fixed_params["base_hidden"]).to(device)
                
                if method == 'fine_tuning':
                    final_res, ft_time = train_fine_tuning(
                        source_model, target_model,
                        ft_X_tensor, ft_y_tensor,
                        test_X_tensor, test_y_tensor,
                        device
                    )
                    total_time = pretrain_time + ft_time
                    epochs_info = f"Source:{source_epochs},Target:{target_epochs}"
                
                elif method == 'feature_extraction':
                    final_res, ft_time = train_feature_extraction(
                        source_model, target_model,
                        ft_X_tensor, ft_y_tensor,
                        test_X_tensor, test_y_tensor,
                        device
                    )
                    total_time = pretrain_time + ft_time
                    epochs_info = f"Source:{source_epochs},Target:{target_epochs}"
                
                elif method == 'mmd_transfer':
                    final_res, ft_time = train_mmd_transfer(
                        source_model, target_model,
                        source_X_tensor, source_y_tensor,
                        ft_X_tensor, ft_y_tensor,
                        test_X_tensor, test_y_tensor,
                        device
                    )
                    total_time = pretrain_time + ft_time
                    epochs_info = f"Source:{source_epochs},Target:{target_epochs}"
                
                elif method == 'adversarial':
                    final_res, ft_time = train_adversarial(
                        source_model, target_model,
                        source_X_tensor, source_y_tensor,
                        ft_X_tensor, ft_y_tensor,
                        test_X_tensor, test_y_tensor,
                        device
                    )
                    total_time = pretrain_time + ft_time
                    epochs_info = f"Source:{source_epochs},Target:{target_epochs}"
            
            # ä»…æºåŸŸè®­ç»ƒæ¨¡å‹ï¼ˆæ ¹æ®æ–¹æ³•ç¡®å®šæ¨¡å‹ç±»å‹ï¼‰
            else:
                if method == 'tcn_bigru_attention':
                    model = TCNBiGRUAttention(
                        input_size,
                        tcn_channels=fixed_params["tcn_chs"],
                        gru_hidden=fixed_params["gru_hid"],
                        num_heads=fixed_params["gru_heads"]
                    ).to(device)
                    model_name = "TCN-BiGRU-Attention"
                    current_params["Model_Type"] = "TCNBiGRUAttention"  # æ ‡è®°æ¨¡å‹ç±»å‹
                    final_res, total_time = train_source_only_model(
                        model, source_X_tensor, source_y_tensor,
                        test_X_tensor, test_y_tensor, device, model_name
                    )
                
                elif method == 'cnn_lstm':
                    model = CNNLSTM(
                        input_size,
                        cnn_filters=fixed_params["cnn_filts"],
                        lstm_hidden=fixed_params["lstm_hid"]
                    ).to(device)
                    model_name = "CNN-LSTM"
                    current_params["Model_Type"] = "CNNLSTM"  # æ ‡è®°æ¨¡å‹ç±»å‹
                    final_res, total_time = train_source_only_model(
                        model, source_X_tensor, source_y_tensor,
                        test_X_tensor, test_y_tensor, device, model_name
                    )
                
                elif method == 'transformer':
                    model = TransformerModel(
                        input_size,
                        d_model=fixed_params["trans_d"],
                        nhead=fixed_params["trans_head"],
                        num_layers=fixed_params["trans_layers"]
                    ).to(device)
                    model_name = "Transformer"
                    current_params["Model_Type"] = "TransformerModel"  # æ ‡è®°æ¨¡å‹ç±»å‹
                    final_res, total_time = train_source_only_model(
                        model, source_X_tensor, source_y_tensor,
                        test_X_tensor, test_y_tensor, device, model_name
                    )
                
                epochs_info = f"Source:{source_epochs}"
            
            # æ–°å¢ï¼šå°†å½“å‰æ–¹æ³•çš„å‚æ•°æ·»åŠ åˆ°åˆ—è¡¨
            network_params_list.append(current_params)
            
            # 4. ç»“æœä¿å­˜ï¼ˆåŸé€»è¾‘ä¸å˜ï¼‰
            print(f"\nğŸ“Š æ–¹æ³• {method} ç»“æœ | MAE: {final_res['mae']:.4f} | RMSE: {final_res['rmse']:.4f} | R2: {final_res['r2']:.4f}")
            
            pred_path = save_prediction_csv(results_root, target_key, method, final_res)
            print(f"ğŸ’¾ é¢„æµ‹ç»“æœ: {pred_path}")
            
            plot_path = plot_error_curve(results_root, target_key, method, final_res)
            print(f"ğŸ“ˆ è¯¯å·®æ›²çº¿: {plot_path}")
            
            performance = {
                'Dataset': target_key,
                'Model': method.replace('_', ' ').title(),
                'MAE': round(final_res['mae'], 4),
                'RMSE': round(final_res['rmse'], 4),
                'R2': round(final_res['r2'], 4),
                'Train_Time(s)': round(total_time, 2),
                'Batch_Size': batch_size,
                'Epochs': epochs_info,
                'Device': str(device)
            }
            performance_list.append(performance)
            
            if exp_idx == 1 and method_idx == 1:
                save_performance_csv(results_root, performance_list, is_init=True)
            else:
                save_performance_csv(results_root, performance_list[-1:], is_init=False)
        
        print(f"\n{'='*80}\n")
    
    # æ–°å¢ï¼šæ‰€æœ‰å®éªŒç»“æŸåï¼Œä¿å­˜ç½‘ç»œæ¨¡å‹å‚æ•°CSV
    save_network_params_csv(results_root, network_params_list)
    
    print(f"ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ€»ç›®å½•: {results_root}")
    print(f"ğŸ“„ æ€§èƒ½æ±‡æ€»è¡¨: {os.path.join(results_root, 'all_models_performance.csv')}")
    print(f"ğŸ“„ ç½‘ç»œå‚æ•°è¡¨: {os.path.join(results_root, 'network_model_parameters.csv')}")


# -------------------------- 7. å¯åŠ¨å®éªŒ --------------------------
if __name__ == "__main__":
    start_time = time.time()
    run_all_experiments()
    total_time = time.time() - start_time
    print(f"\nâ±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
